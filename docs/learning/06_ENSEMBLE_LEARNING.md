# üé≠ Ensemble Learning: Combinando M√∫ltiples Modelos

## ¬øQu√© es Ensemble Learning?

**Ensemble Learning** significa usar **m√∫ltiples modelos juntos** para tomar mejores decisiones que un solo modelo.

> üí° **Analog√≠a**: Es como una junta de expertos. En vez de confiar en un solo experto, re√∫nes a varios y tu predicci√≥n es mejor que cualquiera de ellos individualmente.

---

## üß† El Poder de la Diversidad

### El Ejemplo del Millonario

En el concurso "¬øCu√°nto pesa este toro?", 787 personas adivinaron:
- **Promedio de adivinanzas**: 1,197 libras
- **Peso real**: 1,198 libras
- **Error**: Solo 1 libra (0.08%)

```
Mejor adivinanza individual: 1,096 libras (error: 102 lbs)
Pero el promedio fue m√°s preciso que cualquier individuo
```

**En Machine Learning pasa lo mismo**: La predicci√≥n promedio de m√∫ltiples modelos suele ser mejor que cualquiera individualmente.

---

## üìä M√©todos de Ensemble

### 1. **Voting (Votaci√≥n)**

Cada modelo "vota" y la predicci√≥n final es el resultado m√°s votado.

```
Entrada: Arsenal vs Chelsea

    Model 1 (XGBoost):   HOME WIN     ‚Üí voto 1
    Model 2 (LightGBM):  HOME WIN     ‚Üí voto 1
    Model 3 (Random):    DRAW         ‚Üí voto 0
    
    Resultado: 2 votos HOME WIN
    Predicci√≥n Final: HOME WIN
```

### C√≥digo: Voting Classifier

```python
from sklearn.ensemble import VotingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier

# Crear modelos individuales
xgb = XGBClassifier(n_estimators=100)
lgb = LGBMClassifier(n_estimators=100)
rf = RandomForestClassifier(n_estimators=100)

# Crear ensemble (votaci√≥n dura)
voting = VotingClassifier(
    estimators=[
        ('xgb', xgb),
        ('lgb', lgb),
        ('rf', rf)
    ],
    voting='hard'  # Mayor√≠a de votos
)

voting.fit(X_train, y_train)
predictions = voting.predict(X_test)

# Para probabilidades (voting suave)
voting_soft = VotingClassifier(
    estimators=[('xgb', xgb), ('lgb', lgb), ('rf', rf)],
    voting='soft'  # Promedio de probabilidades
)

probabilities = voting_soft.predict_proba(X_test)
```

### Hard vs Soft Voting

```
Hard Voting (Mayor√≠a):
  Model 1: HOME WIN (probabilidad: 0.7)
  Model 2: HOME WIN (probabilidad: 0.55)
  Model 3: DRAW    (probabilidad: 0.4)
  
  Resultado: HOME WIN (gana por 2 votos)
  
Soft Voting (Promedio de probabilidades):
  HOME WIN: (0.7 + 0.55 + 0.2) / 3 = 0.483
  DRAW:     (0.2 + 0.35 + 0.4) / 3 = 0.317
  AWAY:     (0.1 + 0.1 + 0.4) / 3 = 0.200
  
  Resultado: HOME WIN (probabilidad m√°s alta)
```

---

### 2. **Stacking (Apilamiento)**

Los modelos b√°sicos entrenan, luego sus predicciones se usan como features para un modelo "meta" que aprende c√≥mo combinarlos √≥ptimamente.

```
NIVEL 0 (Base Models):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ XGBoost  ‚îÇ    ‚îÇ LightGBM ‚îÇ    ‚îÇ RF       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ Pred: 0.65    ‚îÇ Pred: 0.58    ‚îÇ Pred: 0.55
      ‚îÇ               ‚îÇ               ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
        Features Nivel 1: [0.65, 0.58, 0.55]
                      ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ                               ‚îÇ
      ‚îÇ     NIVEL 1 (Meta-Learner)    ‚îÇ
      ‚îÇ        (e.g., Logistic)       ‚îÇ
      ‚îÇ                               ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
            Predicci√≥n Final: HOME WIN
```

### C√≥digo: Stacking

```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression

# Modelos base
base_learners = [
    ('xgb', XGBClassifier(n_estimators=50)),
    ('lgb', LGBMClassifier(n_estimators=50)),
    ('rf', RandomForestClassifier(n_estimators=50))
]

# Meta-learner (aprende c√≥mo combinar)
meta_learner = LogisticRegression(max_iter=1000)

# Crear stacking
stacking = StackingClassifier(
    estimators=base_learners,
    final_estimator=meta_learner,
    cv=5  # Cross-validation para generar features
)

stacking.fit(X_train, y_train)
predictions = stacking.predict(X_test)
probabilities = stacking.predict_proba(X_test)
```

### Ventajas del Stacking

```python
# El meta-learner aprende pesos autom√°ticamente
# En lugar de voto igual (1/3 cada uno):
# XGBoost:  40%  (mejor modelo)
# LightGBM: 35%  
# RF:       25%  (menos confiable)
```

---

### 3. **Blending**

Similar al Stacking pero m√°s simple: divide datos en 3 partes.

```
Data
‚îú‚îÄ‚îÄ Train Set 1 (60%)
‚îÇ   ‚îî‚îÄ> Entrena modelos base
‚îÇ
‚îú‚îÄ‚îÄ Train Set 2 (20%)
‚îÇ   ‚îî‚îÄ> Genera predicciones (features para meta-learner)
‚îÇ
‚îî‚îÄ‚îÄ Test Set (20%)
    ‚îî‚îÄ> Evaluaci√≥n final
```

### C√≥digo: Blending

```python
from sklearn.model_selection import train_test_split

# Dividir datos
X_train_base, X_blend, y_train_base, y_blend = train_test_split(
    X_train, y_train, test_size=0.3, random_state=42
)

# Entrenar modelos base
xgb.fit(X_train_base, y_train_base)
lgb.fit(X_train_base, y_train_base)
rf.fit(X_train_base, y_train_base)

# Generar predicciones blend
blend_preds_xgb = xgb.predict_proba(X_blend)
blend_preds_lgb = lgb.predict_proba(X_blend)
blend_preds_rf = rf.predict_proba(X_blend)

# Crear features de blending
X_blend_meta = np.hstack([
    blend_preds_xgb,
    blend_preds_lgb,
    blend_preds_rf
])

# Entrenar meta-learner
meta = LogisticRegression()
meta.fit(X_blend_meta, y_blend)

# Predecir en test
test_preds_xgb = xgb.predict_proba(X_test)
test_preds_lgb = lgb.predict_proba(X_test)
test_preds_rf = rf.predict_proba(X_test)

X_test_meta = np.hstack([
    test_preds_xgb,
    test_preds_lgb,
    test_preds_rf
])

predictions = meta.predict(X_test_meta)
```

---

### 4. **Boosting**

Entrenar modelos secuencialmente, donde cada uno **corrige los errores del anterior**.

```
Iteraci√≥n 1: Entrena Model 1 (comete errores)
                    ‚Üì
Iteraci√≥n 2: Entrena Model 2 (enfocado en errores de M1)
                    ‚Üì
Iteraci√≥n 3: Entrena Model 3 (enfocado en errores de M2)
                    ‚Üì
Predicci√≥n: Suma ponderada de predicciones
```

Ya vimos esto en XGBoost y LightGBM (son m√©todos de boosting).

---

### 5. **Bagging**

Entrenar modelos en paralelo con **subconjuntos aleatorios** de datos.

```
Dataset Original (1000 muestras)
        ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
    ‚ñº   ‚ñº   ‚ñº   ‚ñº   ‚ñº
  Boot Boot Boot Boot Boot
   Set1 Set2 Set3 Set4 Set5
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
  [Train] [Train] [Train] [Train] [Train]
  Model1  Model2  Model3  Model4  Model5
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
    Predicci√≥n Final
    (Promedio o Voto)
```

Random Forest y Extra Trees son ejemplos de Bagging.

---

## üìà Mejora de Precisi√≥n

### Ejemplo Real: Predicci√≥n Match Result (1X2)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Modelo Individual        Precisi√≥n       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ XGBoost                  55.28%          ‚îÇ
‚îÇ LightGBM                 55.49%          ‚îÇ
‚îÇ Random Forest            52.80%          ‚îÇ
‚îÇ CatBoost                 54.80%          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Voting (Hard)            56.15%   (+0.66%)‚îÇ
‚îÇ Voting (Soft)            56.42%   (+0.93%)‚îÇ
‚îÇ Stacking                 56.85%   (+1.36%)‚îÇ
‚îÇ Phase2 (Market+Ensemble) 80.38%   (+25%) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

> üí° El stacking logra superar a cualquier modelo individual

---

## üéØ Cu√°ndo Usar Cada M√©todo

| M√©todo | Velocidad | Precisi√≥n | Complejidad | Caso de Uso |
|--------|-----------|-----------|-------------|------------|
| Voting | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Bajo | Producci√≥n r√°pida |
| Stacking | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Alto | M√°xima precisi√≥n |
| Blending | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Medio | Balance datos |
| Boosting | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Medio | Secuencial |
| Bagging | ‚ö° | ‚≠ê‚≠ê‚≠ê | Medio | Robustez |

---

## üîß Implementaci√≥n Recomendada para EPL-Predict

```python
from sklearn.ensemble import (
    VotingClassifier, 
    StackingClassifier
)
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

class EnsemblePredictor:
    def __init__(self):
        # Modelos base: Diversidad
        self.xgb = XGBClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5
        )
        
        self.lgb = LGBMClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5
        )
        
        self.rf = RandomForestClassifier(
            n_estimators=100,
            max_depth=10
        )
        
        # Ensemble
        self.ensemble = StackingClassifier(
            estimators=[
                ('xgb', self.xgb),
                ('lgb', self.lgb),
                ('rf', self.rf)
            ],
            final_estimator=LogisticRegression(max_iter=1000),
            cv=5
        )
    
    def fit(self, X_train, y_train):
        self.ensemble.fit(X_train, y_train)
        return self
    
    def predict(self, X_test):
        return self.ensemble.predict(X_test)
    
    def predict_proba(self, X_test):
        return self.ensemble.predict_proba(X_test)
```

---

## üìä Monitoreo de Ensemble

```python
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    roc_auc_score
)

# Evaluaci√≥n individual
print("Modelos individuales:")
print(f"XGBoost:   {accuracy_score(y_test, xgb.predict(X_test)):.2%}")
print(f"LightGBM:  {accuracy_score(y_test, lgb.predict(X_test)):.2%}")
print(f"RF:        {accuracy_score(y_test, rf.predict(X_test)):.2%}")

print("\nEnsemble:")
print(f"Ensemble:  {accuracy_score(y_test, ensemble.predict(X_test)):.2%}")

print("\nDetalle:")
print(classification_report(y_test, ensemble.predict(X_test)))
```

---

## üö® Errores Comunes

### ‚ùå **Usar Modelos Muy Similares**

```python
# ‚ùå MAL: Todos son Gradient Boosting
ensemble = VotingClassifier(estimators=[
    ('xgb', XGBClassifier()),
    ('lgb', LGBMClassifier()),
    ('cat', CatBoostClassifier())
])

# ‚úÖ BIEN: Mezclar algoritmos diferentes
ensemble = VotingClassifier(estimators=[
    ('xgb', XGBClassifier()),
    ('rf', RandomForestClassifier()),
    ('svm', SVC(probability=True))
])
```

### ‚ùå **Overfitting del Meta-Learner**

```python
# ‚ùå MAL: Meta-learner muy complejo
meta = XGBClassifier(max_depth=10, n_estimators=200)

# ‚úÖ BIEN: Meta-learner simple
meta = LogisticRegression(C=1.0)
```

### ‚ùå **Data Leakage en Stacking**

```python
# ‚ùå MAL: Entrenar meta-learner con predicciones de train
train_preds = xgb.predict_proba(X_train)
meta.fit(train_preds, y_train)

# ‚úÖ BIEN: Usar cross-validation
# Autom√°tico en StackingClassifier con cv=5
```

---

## üìö Comparaci√≥n: EPL-Predict Phase 2

El proyecto usa un ensemble sofisticado que combina:
- **3 modelos base** (XGBoost, LightGBM, Random Forest)
- **Features de mercado** (odds de casas de apuestas)
- **Meta-learner** (Logistic Regression)

**Resultado**: 80.38% de precisi√≥n (vs 55% individual)

---

## üìö Recursos

- [Scikit-learn Ensemble](https://scikit-learn.org/stable/modules/ensemble.html)
- [XGBoost Ensemble](https://xgboost.readthedocs.io/)
- "Ensemble Methods" - Zhou Zhihua

---

## üöÄ Siguiente Paso

Contin√∫a con [07_VALUE_BETTING_ODDS.md](07_VALUE_BETTING_ODDS.md) para aprender la matem√°tica de las apuestas de valor.
