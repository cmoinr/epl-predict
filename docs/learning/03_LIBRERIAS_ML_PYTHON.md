# üì¶ Librer√≠as de Python para Machine Learning

Este documento explica las librer√≠as utilizadas en el proyecto y sus funciones principales.

---

## üî¢ NumPy - Computaci√≥n Num√©rica

### ¬øQu√© es?
**NumPy** (Numerical Python) es la librer√≠a fundamental para computaci√≥n cient√≠fica. Proporciona arrays multidimensionales eficientes y funciones matem√°ticas.

### ¬øPor qu√© es importante?
- Los modelos de ML trabajan con **matrices num√©ricas**, no con listas de Python
- Es ~50x m√°s r√°pido que las listas tradicionales
- Es la base de casi todas las dem√°s librer√≠as de ML

### Uso en el proyecto

```python
import numpy as np

# Crear array de features (de predictor.py)
features = np.array([
    home_shots,           # 0
    away_shots,           # 1
    home_shots_on_target, # 2
    # ... 28 features en total
]).reshape(1, -1)  # Reshape a (1, 28) para una predicci√≥n

# Operaciones comunes
np.mean(team_probs)          # Promedio
np.abs(array1 - array2)      # Valor absoluto de diferencias
np.clip(value, 0, 1)         # Limitar valores entre 0 y 1
np.nan_to_num(features)      # Reemplazar NaN con 0
np.hstack([arr1, arr2])      # Concatenar horizontalmente
```

### Conceptos clave

```python
# Shape (forma del array)
array = np.array([[1,2,3], [4,5,6]])
print(array.shape)  # (2, 3) ‚Üí 2 filas, 3 columnas

# Reshape (cambiar forma)
flat = np.array([1,2,3,4,5,6])
matrix = flat.reshape(2, 3)   # Convertir a 2x3
prediction = flat.reshape(1, -1)  # 1 fila, columnas autom√°ticas
```

---

## üêº Pandas - Manipulaci√≥n de Datos

### ¬øQu√© es?
**Pandas** es la librer√≠a principal para an√°lisis y manipulaci√≥n de datos tabulares (como Excel, pero programable).

### Estructuras principales

```python
import pandas as pd

# DataFrame: tabla con filas y columnas nombradas
df = pd.read_csv('data/raw/epl_final.csv')

# Series: una columna individual
goals = df['FullTimeHomeGoals']  # Serie de goles locales
```

### Uso en el proyecto

```python
# feature_engineering.py

# 1. Cargar y ordenar datos
df = pd.read_csv('epl_final.csv')
df['MatchDate'] = pd.to_datetime(df['MatchDate'])
df = df.sort_values('MatchDate').reset_index(drop=True)

# 2. Filtrar datos
home_matches = df[df['HomeTeam'] == 'Arsenal']
recent_matches = df[df['MatchDate'] >= '2024-01-01']

# 3. Operaciones de rolling (ventanas m√≥viles)
# Promedio de goles en √∫ltimos 10 partidos
df['AvgGoals_L10'] = df['FullTimeHomeGoals'].rolling(
    window=10, 
    min_periods=1
).mean().shift(1)  # shift(1) evita data leakage

# 4. Mapeo de categor√≠as
result_map = {'A': 0, 'D': 1, 'H': 2}
y = df['FullTimeResult'].map(result_map)

# 5. Manejo de valores faltantes
df_filled = df.fillna(method='ffill')  # Forward fill
df_filled = df.fillna(df.mean())       # Llenar con promedio

# 6. Concatenar DataFrames
combined = pd.concat([df1, df2], axis=0)  # Verticalmente
combined = pd.concat([df1, df2], axis=1)  # Horizontalmente
```

### Rolling Windows (Ventanas M√≥viles)
Concepto muy usado para features temporales:

```python
# Calcular forma del equipo (√∫ltimos 5 partidos)
#
# Partido 1: Gan√≥ (3 pts)
# Partido 2: Empat√≥ (1 pt)
# Partido 3: Perdi√≥ (0 pts)
# Partido 4: Gan√≥ (3 pts)
# Partido 5: Gan√≥ (3 pts)
# 
# Rolling(5).mean() = (3+1+0+3+3)/5 = 2.0 puntos promedio
```

---

## üî¨ Scikit-learn - Machine Learning

### ¬øQu√© es?
**Scikit-learn** es LA librer√≠a est√°ndar de ML en Python. Proporciona algoritmos, preprocesamiento y evaluaci√≥n.

### M√≥dulos principales usados en el proyecto

```python
from sklearn.ensemble import (
    RandomForestClassifier,      # Clasificaci√≥n con Random Forest
    RandomForestRegressor,       # Regresi√≥n con Random Forest
    GradientBoostingClassifier,  # Clasificaci√≥n con Gradient Boosting
    GradientBoostingRegressor,   # Regresi√≥n con Gradient Boosting
    VotingClassifier,            # Ensemble de votaci√≥n
    VotingRegressor
)

from sklearn.preprocessing import StandardScaler  # Normalizaci√≥n
from sklearn.model_selection import train_test_split  # Divisi√≥n de datos
from sklearn.metrics import (
    accuracy_score,       # Precisi√≥n de clasificaci√≥n
    mean_absolute_error,  # Error absoluto medio
    classification_report # Reporte detallado
)
```

### Flujo t√≠pico con Scikit-learn

```python
# 1. Preparar datos
X = df[feature_columns]
y = df['target']

# 2. Dividir en train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,     # 20% para test
    random_state=42    # Reproducibilidad
)

# 3. Escalar features (IMPORTANTE)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Aprende y transforma
X_test_scaled = scaler.transform(X_test)        # Solo transforma

# 4. Entrenar modelo
model = RandomForestClassifier(n_estimators=200)
model.fit(X_train_scaled, y_train)

# 5. Predecir
predictions = model.predict(X_test_scaled)
probabilities = model.predict_proba(X_test_scaled)

# 6. Evaluar
accuracy = accuracy_score(y_test, predictions)
```

### StandardScaler - Normalizaci√≥n

¬øPor qu√© escalar las features?

```python
# Sin escalar:
# HomeShots: 0-30 (rango peque√±o)
# TotalGoalsHistorico: 0-5000 (rango enorme)
# 
# El modelo dar√≠a m√°s peso a TotalGoalsHistorico solo por su escala

# Con StandardScaler:
# Todas las features tienen media=0 y desviaci√≥n est√°ndar=1
# HomeShots: -2.5 a 2.5
# TotalGoalsHistorico: -2.5 a 2.5
```

---

## ‚ö° XGBoost - Gradient Boosting Extremo

### ¬øQu√© es?
**XGBoost** (eXtreme Gradient Boosting) es una implementaci√≥n optimizada de Gradient Boosting. Es conocido por ganar muchas competencias de Kaggle.

### Uso en el proyecto

```python
from xgboost import XGBClassifier, XGBRegressor

# Clasificaci√≥n de resultado
xgb_result = XGBClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    random_state=42,
    use_label_encoder=False,
    eval_metric='mlogloss'
)
xgb_result.fit(X_train_scaled, y_train)

# Predicci√≥n
pred = xgb_result.predict(X_test_scaled)
prob = xgb_result.predict_proba(X_test_scaled)
```

### ¬øPor qu√© XGBoost es popular?
- **R√°pido**: Paralelizaci√≥n y optimizaciones
- **Preciso**: Regularizaci√≥n incorporada contra overfitting
- **Flexible**: Maneja valores faltantes autom√°ticamente

---

## üí° LightGBM - Gradient Boosting Ligero

### ¬øQu√© es?
**LightGBM** (Light Gradient Boosting Machine) es de Microsoft. Es m√°s r√°pido que XGBoost especialmente con datasets grandes.

### Uso en el proyecto

```python
from lightgbm import LGBMClassifier, LGBMRegressor

lgb_result = LGBMClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    random_state=42,
    verbose=-1  # Silenciar logs
)
lgb_result.fit(X_train_scaled, y_train)
```

### Diferencia con XGBoost
- LightGBM crece los √°rboles **hoja a hoja** (leaf-wise)
- XGBoost crece **nivel a nivel** (level-wise)
- LightGBM es m√°s r√°pido pero puede overfit m√°s f√°cilmente

---

## üê± CatBoost - Gradient Boosting para Categ√≥ricos

### ¬øQu√© es?
**CatBoost** (Categorical Boosting) es de Yandex. Maneja variables categ√≥ricas de forma nativa sin necesidad de encoding.

### Uso en el proyecto

```python
from catboost import CatBoostClassifier, CatBoostRegressor

cat_result = CatBoostClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    random_state=42,
    verbose=0
)
cat_result.fit(X_train_scaled, y_train)
```

### Ventaja principal
Si tienes features como "HomeTeam" o "Season", CatBoost las maneja directamente sin convertirlas a n√∫meros.

---

## üóÉÔ∏è Pickle - Serializaci√≥n de Modelos

### ¬øQu√© es?
**Pickle** es el m√≥dulo de Python para **serializar** (guardar) objetos en archivos binarios y cargarlos despu√©s.

### Uso en el proyecto

```python
import pickle

# GUARDAR modelo entrenado
with open('models/rf_result_model.pkl', 'wb') as f:
    pickle.dump(trained_model, f)

# CARGAR modelo guardado (en predictor.py)
with open('models/rf_result_model.pkl', 'rb') as f:
    model = pickle.load(f)

# Ahora puedes usar el modelo sin reentrenar
predictions = model.predict(new_data)
```

### Modelos guardados en el proyecto
```
models/
‚îú‚îÄ‚îÄ rf_result_model.pkl       # Random Forest - Resultado
‚îú‚îÄ‚îÄ gb_result_model.pkl       # Gradient Boosting - Resultado
‚îú‚îÄ‚îÄ xgb_result_model.pkl      # XGBoost - Resultado
‚îú‚îÄ‚îÄ lgb_result_model.pkl      # LightGBM - Resultado
‚îú‚îÄ‚îÄ cat_result_model.pkl      # CatBoost - Resultado
‚îú‚îÄ‚îÄ voting_result_model.pkl   # Voting Ensemble - Resultado
‚îú‚îÄ‚îÄ rf_goals_model.pkl        # Random Forest - Goles
‚îú‚îÄ‚îÄ rf_btts_model.pkl         # Random Forest - BTTS
‚îú‚îÄ‚îÄ scaler_model.pkl          # StandardScaler
‚îî‚îÄ‚îÄ phase2_voting_market.pkl  # Modelo con Market Intelligence
```

---

## üìä Resumen de Librer√≠as

| Librer√≠a | Prop√≥sito | Funci√≥n Principal |
|----------|-----------|-------------------|
| **NumPy** | Computaci√≥n num√©rica | Arrays, operaciones matem√°ticas |
| **Pandas** | Manipulaci√≥n de datos | DataFrames, limpieza, transformaci√≥n |
| **Scikit-learn** | ML general | Modelos, preprocesamiento, evaluaci√≥n |
| **XGBoost** | Gradient Boosting | Modelos de alta precisi√≥n |
| **LightGBM** | Gradient Boosting r√°pido | Datasets grandes |
| **CatBoost** | GB para categ√≥ricos | Features categ√≥ricas nativas |
| **Pickle** | Serializaci√≥n | Guardar/cargar modelos |

---

## üöÄ Siguiente Paso

Contin√∫a con [04_FEATURE_ENGINEERING.md](04_FEATURE_ENGINEERING.md) para aprender c√≥mo crear las variables que alimentan a estos modelos.
