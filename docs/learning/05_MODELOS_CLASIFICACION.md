# ğŸ¤– Modelos de ClasificaciÃ³n en Machine Learning

## Â¿QuÃ© es ClasificaciÃ³n?

La **clasificaciÃ³n** es una tarea de Machine Learning donde predecimos **categorÃ­as discretas** (no valores continuos).

```
REGRESIÃ“N (Valores continuos)          CLASIFICACIÃ“N (CategorÃ­as)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Predecir: 2.5 goles                     Predecir: Home Win / Draw / Away Win
Predecir: temperatura 15.3Â°C            Predecir: Lluvia / No lluvia
Predecir: precio $1,250                 Predecir: Spam / No Spam
```

---

## ğŸ¯ Problemas de ClasificaciÃ³n en EPL-Predict

### 1. **Match Result (1X2)**
```
Entrada (Features):
â”œâ”€â”€ home_form_5: 0.667
â”œâ”€â”€ away_form_5: 0.733
â”œâ”€â”€ h2h_home_wins: 0.60
â””â”€â”€ position_diff: -0.10

Salida (Target):
â”œâ”€â”€ Home Win (1)
â”œâ”€â”€ Draw (X)
â””â”€â”€ Away Win (2)
```

**PrecisiÃ³n actual**: 55.3%

### 2. **Both Teams To Score (BTTS)**
```
Entrada: Features del partido

Salida:
â”œâ”€â”€ SÃ­ (ambos equipos anotan)
â””â”€â”€ No (un equipo no anota)
```

**PrecisiÃ³n actual**: 78.37%

---

## ğŸŒ³ Random Forest (Bosques Aleatorios)

### Concepto BÃ¡sico

Un **Random Forest** es como una **junta de directivos**:
- Entrenan mÃºltiples Ã¡rboles de decisiÃ³n
- Cada Ã¡rbol vota por una clase
- La predicciÃ³n final es el resultado mÃ¡s votado

```
                          Random Forest
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚               â”‚               â”‚
            Ãrbol 1          Ãrbol 2         Ãrbol 3
              â”‚                â”‚               â”‚
         Predice: H        Predice: H      Predice: X
              â”‚                â”‚               â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                      VotaciÃ³n: 2 votos H, 1 voto X
                      Resultado Final: HOME WIN
```

### Estructura de un Ãrbol de DecisiÃ³n

```
                    Â¿home_form_5 > 0.65?
                         /    \
                      SÃ­ /      \ No
                       /          \
                      /            \
                      
         Â¿h2h_home > 0.5?      Â¿position_diff > 0.2?
           /    \                 /    \
        SÃ­ /      \ No         SÃ­ /      \ No
         /          \          /          \
     HOME WIN     DRAW      AWAY WIN    DRAW
```

### CÃ³digo

```python
from sklearn.ensemble import RandomForestClassifier

# 1. Crear modelo
rf = RandomForestClassifier(
    n_estimators=100,      # 100 Ã¡rboles
    max_depth=10,          # Profundidad mÃ¡xima
    min_samples_split=5,   # MÃ­nimo de muestras para dividir
    random_state=42
)

# 2. Entrenar
rf.fit(X_train, y_train)

# 3. Predecir
predictions = rf.predict(X_test)
probabilities = rf.predict_proba(X_test)  # Probabilidades por clase

# 4. Evaluar
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, predictions)
print(f"PrecisiÃ³n: {accuracy:.2%}")
```

### Ventajas & Desventajas

| Ventajas | Desventajas |
|----------|-------------|
| âœ… Maneja features no lineales | âŒ Puede hacer overfitting |
| âœ… Naturalmente multiclase | âŒ Lento con muchos features |
| âœ… Importancia de features | âŒ DifÃ­cil de interpretar |
| âœ… Robusto a outliers | âŒ Requiere mÃ¡s memoria |

**PrecisiÃ³n en EPL**: 52.8%

---

## ğŸš€ Gradient Boosting

### Concepto

**Gradient Boosting** es como **aprender de los errores**:
1. Entrena un primer Ã¡rbol (comete errores)
2. El siguiente Ã¡rbol **intenta corregir** esos errores
3. Repite N veces, mejorando paso a paso

```
PredicciÃ³n Real: Home Win
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Ãrbol 1: Predice Draw (ERROR: -0.33)
  â””â”€> Siguiente Ã¡rbol enfocado en corregir este error

Ãrbol 2: Predice Home Win (reduce error a -0.05)
  â””â”€> Siguiente Ã¡rbol sigue mejorando

Ãrbol 3: Predice Home Win (error casi 0)
  â””â”€> PredicciÃ³n final: Home Win âœ“
```

### CÃ³digo

```python
from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(
    n_estimators=100,        # 100 etapas de boosting
    learning_rate=0.1,       # Velocidad de aprendizaje
    max_depth=5,             # Ãrboles mÃ¡s superficiales
    subsample=0.8            # Usa 80% de datos por etapa
)

gb.fit(X_train, y_train)
predictions = gb.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
```

### Ventajas & Desventajas

| Ventajas | Desventajas |
|----------|-------------|
| âœ… Muy preciso | âŒ Riesgo de overfitting |
| âœ… Maneja datos complejos | âŒ Lento entrenamiento |
| âœ… Importancia de features | âŒ Requiere tuning cuidadoso |

**PrecisiÃ³n en EPL**: 55.33%

---

## âš¡ XGBoost (eXtreme Gradient Boosting)

### Â¿QuÃ© lo hace diferente?

XGBoost es una **versiÃ³n mejorada y optimizada** de Gradient Boosting:
- MÃ¡s rÃ¡pido (optimizado en C++)
- Mejor control de regularizaciÃ³n
- Mejor manejo de datos desbalanceados
- Manejo automÃ¡tico de valores faltantes

### CÃ³digo

```python
from xgboost import XGBClassifier

xgb = XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,    # Usa 80% de features
    objective='multi:softmax', # Multiclase (1X2)
    num_class=3,             # 3 clases (H, X, A)
    random_state=42,
    eval_metric='mlogloss'    # MÃ©trica de evaluaciÃ³n
)

xgb.fit(X_train, y_train)
predictions = xgb.predict(X_test)
```

### ParÃ¡metros Importantes

```python
# RegularizaciÃ³n (evita overfitting)
lambda = 1.0         # L2 regularization (Ridge)
alpha = 0.0          # L1 regularization (Lasso)
gamma = 0.0          # Penalidad por complejidad

# Crecimiento del Ã¡rbol
max_depth = 5        # Profundidad mÃ¡xima
min_child_weight = 1 # Peso mÃ­nimo en hoja

# Learning
learning_rate = 0.1  # TamaÃ±o del paso
n_estimators = 100   # NÃºmero de Ã¡rboles
```

**PrecisiÃ³n en EPL**: 55.28%

---

## ğŸŒŸ LightGBM (Light Gradient Boosting Machine)

### CaracterÃ­sticas Principales

LightGBM es aÃºn mÃ¡s **rÃ¡pido y eficiente** que XGBoost:
- 10-20x mÃ¡s rÃ¡pido en entrenamiento
- Usa menos memoria
- Excelente con datos grandes
- Leaf-wise tree growth (crece donde mÃ¡s lo necesita)

```
XGBoost (Level-wise):        LightGBM (Leaf-wise):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Root                         Root
    / \                          / \
   /   \                        /   \
  /     \                      /     \
 /       \          vs       /       \
/_________ \                /         \
 /\   /\                   /\       /\
                                   (Crece mÃ¡s eficiente)
```

### CÃ³digo

```python
from lightgbm import LGBMClassifier

lgb = LGBMClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    num_leaves=31,           # MÃ¡ximo de hojas
    subsample=0.8,
    colsample_bytree=0.8,
    boosting_type='gbdt',    # Gradient Boosting Decision Tree
    num_class=3,
    objective='multiclass',
    metric='multi_logloss'
)

lgb.fit(X_train, y_train)
predictions = lgb.predict(X_test)
```

**PrecisiÃ³n en EPL**: 55.49%

---

## ğŸ± CatBoost (Categorical Boosting)

### Especialidad: Datos CategÃ³ricos

CatBoost estÃ¡ **optimizado para manejar variables categÃ³ricas** automÃ¡ticamente:
- No requiere one-hot encoding
- Maneja la codificaciÃ³n automÃ¡ticamente
- Menos propenso a overfitting
- Resultados mÃ¡s consistentes

### CÃ³digo

```python
from catboost import CatBoostClassifier

cat = CatBoostClassifier(
    iterations=100,
    learning_rate=0.1,
    max_depth=5,
    verbose=10,              # Mostrar progreso
    cat_features=['HomeTeam', 'AwayTeam'],  # Features categÃ³ricas
    loss_function='MultiClass',
    random_state=42
)

# Nota: Puede pasar strings directamente
cat.fit(X_train, y_train, cat_features=cat_features_indices)
predictions = cat.predict(X_test)
```

**PrecisiÃ³n en EPL**: Similar a XGBoost (55%+)

---

## ğŸ“Š ComparaciÃ³n de Modelos

### PrecisiÃ³n en PredicciÃ³n de Resultados (1X2)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Modelo                  PrecisiÃ³n    Tiempo      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Naive Baseline           33.3%      < 1ms       â”‚
â”‚ Logistic Regression      48.5%      10ms        â”‚
â”‚ Random Forest            52.8%      500ms       â”‚
â”‚ Gradient Boosting        55.33%     1500ms      â”‚
â”‚ XGBoost                  55.28%     800ms       â”‚
â”‚ LightGBM                 55.49%     300ms       â”‚
â”‚ CatBoost                 54.8%      1000ms      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Trade-off: PrecisiÃ³n vs Velocidad

```
         PrecisiÃ³n
            â–²
            â”‚     CatBoost LightGBM
            â”‚         â—â—
            â”‚    Gradient Boosting â—
            â”‚        XGBoost â—
         55%â”œâ”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—
            â”‚    Random Forest
            â”‚        â—
            â”‚
         50%â”œâ”€â”€â”€â”€â”€â—
            â”‚   Logistic Regression
            â”‚
         45%â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
              100ms  500ms  1000ms  2000ms      Tiempo
```

---

## ğŸ¯ MÃ©tricas de EvaluaciÃ³n

### Matriz de ConfusiÃ³n (ClasificaciÃ³n Binaria)

```
              PredicciÃ³n
              Positivo  Negativo
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Actual   â”‚ TP   | FN           â”‚
Positivo â”‚ (Acierto) | (Error) â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚ FP   | TN           â”‚
Negativo â”‚ (Error) | (Acierto) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TP = True Positive (predijo SÃ­, era SÃ­)
FN = False Negative (predijo No, era SÃ­)
FP = False Positive (predijo SÃ­, era No)
TN = True Negative (predijo No, era No)
```

### MÃ©tricas Principales

```python
from sklearn.metrics import (
    accuracy_score,          # (TP+TN) / Total
    precision_score,         # TP / (TP+FP) - Exactitud
    recall_score,            # TP / (TP+FN) - Sensibilidad
    f1_score,                # Media armÃ³nica
    roc_auc_score,           # Ãrea bajo curva ROC
    confusion_matrix         # Matriz de confusiÃ³n
)

# Calcular
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)

print(f"Accuracy: {accuracy_score(y_test, y_pred):.2%}")
print(f"Precision: {precision_score(y_test, y_pred):.2%}")
print(f"Recall: {recall_score(y_test, y_pred):.2%}")
print(f"F1-Score: {f1_score(y_test, y_pred):.2%}")
print(f"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.2%}")
```

---

## ğŸ”§ Hyperparameter Tuning

### Grid Search (BÃºsqueda en Grilla)

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2]
}

grid = GridSearchCV(
    XGBClassifier(),
    param_grid,
    cv=5,              # 5-fold cross validation
    scoring='accuracy'
)

grid.fit(X_train, y_train)
print(f"Mejor precisiÃ³n: {grid.best_score_:.2%}")
print(f"Mejores parÃ¡metros: {grid.best_params_}")
```

### Random Search (BÃºsqueda Aleatoria)

```python
from sklearn.model_selection import RandomizedSearchCV

random_search = RandomizedSearchCV(
    XGBClassifier(),
    param_grid,
    n_iter=20,  # 20 combinaciones aleatorias
    cv=5
)

random_search.fit(X_train, y_train)
```

---

## ğŸš¨ Overfitting vs Underfitting

### El TriÃ¡ngulo del Aprendizaje

```
        Performance
            â–²
            â”‚
    Bajo   â”‚ â•±â•²  Perfect
   Sesgo   â”‚â•±  â•²  Balance
            â”‚    â•²
            â”‚     â•²___ Overfitting
            â”‚         (memoriza datos)
            â”‚        â•±
            â”‚______â•± Underfitting
            â”‚     (muy simple)
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Complejidad Modelo
```

### SeÃ±ales de Overfitting

```
Train Accuracy: 98%  â†â”€ Muy alto
Test Accuracy:  52%  â†â”€ Mucho mÃ¡s bajo

Diferencia > 10%: Probable overfitting
```

### Soluciones

```python
# 1. Aumentar regularizaciÃ³n
xgb = XGBClassifier(
    lambda=2.0,      # Aumentar
    alpha=0.5,       # Aumentar
    gamma=1.0        # Aumentar
)

# 2. Reducir complejidad
xgb = XGBClassifier(
    max_depth=3,     # Reducir de 7
    min_child_weight=5  # Aumentar
)

# 3. MÃ¡s datos de entrenamiento
# 4. Early stopping
xgb.fit(X_train, y_train,
        eval_set=[(X_val, y_val)],
        early_stopping_rounds=10)
```

---

## ğŸ“š Recursos

- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [LightGBM Documentation](https://lightgbm.readthedocs.io/)
- [CatBoost Documentation](https://catboost.ai/)
- [Scikit-learn Classification](https://scikit-learn.org/stable/modules/classification.html)

---

## ğŸš€ Siguiente Paso

ContinÃºa con [06_ENSEMBLE_LEARNING.md](06_ENSEMBLE_LEARNING.md) para aprender cÃ³mo combinar mÃºltiples modelos.
